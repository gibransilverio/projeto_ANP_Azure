{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adfd201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LAKE STORAGE - DFS.CORE\n",
    "\n",
    "# \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net\" (Gen 2 Hierarchical Name)\n",
    "# \"wasbs://<container-name>@<storage-account-name>.dfs.core.windows.net\" (Blob)\n",
    "# \"fs.azure.account.key.<storage-account>.dfs.core.windows.net\":\"<storage-account-access-key>\" (Extra_configs)\n",
    "\n",
    "\n",
    "# BLOBO STORAGE - BLOB.CORE\n",
    "\n",
    "# \"abfss://<container-name>@<storage-account-name>.blob.core.windows.net\" (Gen 2 Hierarchical Name)\n",
    "# \"wasbs://<container-name>@<storage-account-name>.blob.core.windows.net\" (Blob)\n",
    "# \"fs.azure.account.key.<storage-account>.blob.core.windows.net\":\"<storage-account-access-key>\" (Extra_configs)\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    " source = \"wasbs://projetodados@storageprojetoengdados.blob.core.windows.net\",\n",
    " mount_point = \"/mnt/projetodados\", \n",
    " extra_configs = {\"fs.azure.account.key.storageprojetoengdados.blob.core.windows.net\":\"L/NL7PxKQ88pOrYwJT1vx+ryH79wo/4TUKN0Ub3yppvzn9DRRH3L4nn7cuPj+fDVeAd4CKrtqX66+AStETjNvg==\"}\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbutils.fs.unmount(\"/mnt/projetodados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/projetodados/raw/dados/2020\"))\n",
    "display(dbutils.fs.ls(\"/mnt/projetodados/raw/dados/2021\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b93bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load = Carregar e criar um dataframe através de um tipo de arquivo, assim passando o caminho do arquivo\n",
    "\n",
    "df_1_sem_2020 = spark.read.format(\"csv\")\\\n",
    "                          .option(\"sep\",\";\")\\\n",
    "                          .option(\"header\",\"True\")\\\n",
    "                          .option(\"inferSchema\",\"True\")\\\n",
    "                          .load(\"/mnt/projetodados/raw/dados/2020/1_sem_2020.csv\")\n",
    "\n",
    "display(df_1_sem_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_sem_2020 = spark.read.format(\"csv\")\\\n",
    "                          .option(\"sep\",\";\")\\\n",
    "                          .option(\"header\",\"True\")\\\n",
    "                          .option(\"inferSchema\",\"True\")\\\n",
    "                          .load(\"/mnt/projetodados/raw/dados/2020/2_sem_2020.csv\")\n",
    "\n",
    "display(df_2_sem_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c62f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_sem_2021 = spark.read.format(\"csv\")\\\n",
    "                          .option(\"sep\",\";\")\\\n",
    "                          .option(\"header\",\"True\")\\\n",
    "                          .option(\"inferSchema\",\"True\")\\\n",
    "                          .load(\"/mnt/projetodados/raw/dados/2021/1_sem_2021.csv\")\n",
    "\n",
    "display(df_1_sem_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73675683",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_sem_2021 = spark.read.format(\"csv\")\\\n",
    "                          .option(\"sep\",\";\")\\\n",
    "                          .option(\"header\",\"True\")\\\n",
    "                          .option(\"inferSchema\",\"True\")\\\n",
    "                          .load(\"/mnt/projetodados/raw/dados/2021/2_sem_2021.csv\")\n",
    "display(df_2_sem_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d31f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Modos para visualizar:\n",
    "# display(df_1_sem_2020.dtypes)\n",
    "# df_1_sem_2020.printSchema()\n",
    "\n",
    "\n",
    "df_1_sem_2020.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aadf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df.union(dataframe_a_se_unido)\n",
    "\n",
    "df_2020 = df_1_sem_2020.union(df_2_sem_2020)\n",
    "df_2021 = df_1_sem_2021.union(df_2_sem_2021)\n",
    "df_full = df_2020.union(df_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8df169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_full.withColumnRenamed(\"Regiao - Sigla\", \"REGIAO\")\\\n",
    "                   .withColumnRenamed(\"Estado - Sigla\", \"ESTADO\")\\\n",
    "                   .withColumnRenamed(\"Municipio\", \"MUNICIPIO\")\\\n",
    "                   .withColumnRenamed(\"Revenda\", \"REVENDA\")\\\n",
    "                   .withColumnRenamed(\"CNPJ da Revenda\", \"CNPJ\")\\\n",
    "                   .withColumnRenamed(\"Nome da Rua\", \"NOME_RUA\")\\\n",
    "                   .withColumnRenamed(\"Numero Rua\", \"NUM_RUA\")\\\n",
    "                   .withColumnRenamed(\"Complemento\", \"COMPL_RUA\")\\\n",
    "                   .withColumnRenamed(\"Bairro\", \"BAIRRO\")\\\n",
    "                   .withColumnRenamed(\"Cep\", \"CEP\")\\\n",
    "                   .withColumnRenamed(\"Produto\", \"PRODUTO\")\\\n",
    "                   .withColumnRenamed(\"Data da Coleta\", \"DT_COLETA\")\\\n",
    "                   .withColumnRenamed(\"Valor de Venda\", \"VL_VENDA\")\\\n",
    "                   .withColumnRenamed(\"Valor de Compra\", \"VL_COMPRA\")\\\n",
    "                   .withColumnRenamed(\"Unidade de Medida\", \"UNID_MED\")\\\n",
    "                   .withColumnRenamed(\"Bandeira\", \"BANDEIRA\")\n",
    "\n",
    "display(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b80aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.createOrReplaceTempView(\"ANP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from ANP;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datatypes = spark.sql(\"\"\" SELECT REGIAO, \n",
    "                                    ESTADO,\n",
    "                                    MUNICIPIO,\n",
    "                                    REVENDA,\n",
    "                                    CNPJ,\n",
    "                                    NOME_RUA,\n",
    "                                    NUM_RUA,\n",
    "                                    COMPL_RUA,\n",
    "                                    BAIRRO,\n",
    "                                    CEP,\n",
    "                                    PRODUTO,\n",
    "                                    TO_DATE(DT_COLETA, 'dd/mm/yyyy')   as DT_COLETA,\n",
    "                                    CAST(REPLACE(VL_VENDA, ',' , '.')  as NUMERIC(4,3)) as  VL_VENDA,\n",
    "                                    CAST(REPLACE(VL_COMPRA,',' , '.')  as NUMERIC(4,3)) AS  VL_COMPRA,\n",
    "                                    UNID_MED,\n",
    "                                    BANDEIRA\n",
    "                                    FROM ANP;\n",
    "\"\"\")\n",
    "\n",
    "display(df_datatypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c781cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datatypes.createOrReplaceTempView(\"ANP_NEW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62be4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from ANP_NEW;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed63623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: DEVE-SE CRIAR PRIMEIRAMENTE A TABELA NO SQL SERVER PARA FAZER A CARGA DO DATASET NO SQL SERVER\n",
    "\n",
    "\n",
    "# Verificando quantidade de linhas do daframe\n",
    "\n",
    "df_datatypes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6922a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitando quantidade de linhas no dataframe com Limit\n",
    "\n",
    "df_limit = spark.sql(\"SELECT * FROM ANP_NEW LIMIT 50\")\n",
    "\n",
    "df_limit.count() # 50 Linhas\n",
    "\n",
    "display(df_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ddd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- Vendo o tamanho de uma coluna \n",
    "-- len() ou lenght()\n",
    "\n",
    "-- select max(lenght(BAIRRO)) FROM ANP_NEW;\n",
    "\n",
    "select max(len(BAIRRO)) FROM ANP_NEW;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e871a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "/*\n",
    "\n",
    "CREATE TABLE TABLE_ANP_50(\n",
    "REGIAO VARCHAR(50) NULL,\n",
    "ESTADO VARCHAR(50) NULL,\n",
    "MUNICIPIO VARCHAR(50) NULL,\n",
    "REVENDA VARCHAR(50) NULL,\n",
    "CNPJ VARCHAR(50) NULL,\n",
    "NOME_RUA VARCHAR(50) NULL,\n",
    "NUM_RUA VARCHAR(50) NULL,\n",
    "COMPL_RUA VARCHAR(50) NULL,\n",
    "BAIRRO VARCHAR(50) NULL,\n",
    "CEP VARCHAR(50) NULL,\n",
    "PRODUTO VARCHAR(50) NULL,\n",
    "DT_COLETA DATE NULL,\n",
    "VL_VENDA NUMERIC(4,3) NULL,\n",
    "VL_COMPRA NUMERIC(4,3) NULL,\n",
    "UNID_MED VARCHAR(50) NULL,\n",
    "BANDEIRA VARCHAR(50) NULL\n",
    ");\n",
    "\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAZENDO CARGA DO DATASET NO SQL SERVER \n",
    "\n",
    "# dbtable = databricks table (nome da tabela criada no sql server)\n",
    "\n",
    "# url = url de conexão do servidor sql server --> projeto-eng-dados-sqlserver.database.windows.net\n",
    "\n",
    "df_limit.write.format(\"jdbc\")\\\n",
    "              .mode(\"append\")\\\n",
    "              .option(\"url\",\"jdbc:sqlserver://projeto-eng-dados-sqlserver.database.windows.net\")\\\n",
    "              .option(\"port\",\"1433\")\\\n",
    "              .option(\"user\", \"gibran\")\\\n",
    "              .option(\"password\",\"Blackggs18#3\")\\\n",
    "              .option(\"database\",\"projeto-eng-dados-databaseql\")\\\n",
    "              .option(\"dbtable\",\"TABLE_ANP_50\")\\\n",
    "              .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABELAS DIMENSÕES - STAR SCHEMA\n",
    "\n",
    "df_regiao = spark.sql(\"\"\" SELECT DISTINCT REGIAO FROM ANP_NEW\n",
    "                          ORDER BY REGIAO ASC;\n",
    "\"\"\")\n",
    "\n",
    "display(df_regiao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df4e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas do Pyspark\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Criando ID incremental com \"monotonically_increasing_id()\"\" ----> Começa no ID 0 \n",
    "\n",
    "df_regiao_id = df_regiao.withColumn(\"ID_REGIAO\", monotonically_increasing_id()+1) # + 1 pois começa no ID 0\n",
    "\n",
    "display(df_regiao_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "/*\n",
    "\n",
    "CREATE SCHEMA DW;\n",
    "\n",
    "CREATE TABLE DW.DIM_REGIAO(\n",
    "ID_REGIAO NUMERIC(1) NOT NULL,\n",
    "REGIAO VARCHAR(2) NULL,\n",
    "PRIMARY KEY (ID_REGIAO));\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regiao_id.write.format(\"jdbc\")\\\n",
    "              .mode(\"append\")\\\n",
    "              .option(\"url\",\"jdbc:sqlserver://projeto-eng-dados-sqlserver.database.windows.net\")\\\n",
    "              .option(\"port\",\"1433\")\\\n",
    "              .option(\"user\", \"gibran\")\\\n",
    "              .option(\"password\",\"Blackggs18#3\")\\\n",
    "              .option(\"database\",\"projeto-eng-dados-databaseql\")\\\n",
    "              .option(\"dbtable\",\"DW.DIM_REGIAO\")\\\n",
    "              .save()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a13915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_dados(df, tabela):\n",
    "    df.write.format(\"jdbc\")\\\n",
    "            .mode(\"append\")\\\n",
    "            .option(\"url\",\"jdbc:sqlserver://projeto-eng-dados-sqlserver.database.windows.net\")\\\n",
    "            .option(\"port\",\"1433\")\\\n",
    "            .option(\"user\", \"gibran\")\\\n",
    "            .option(\"password\",\"Blackggs18#3\")\\\n",
    "            .option(\"database\",\"projeto-eng-dados-databaseql\")\\\n",
    "            .option(\"dbtable\",tabela)\\\n",
    "            .save()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f47de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_estado = spark.sql(\"\"\" SELECT DISTINCT ESTADO FROM ANP_NEW\n",
    "                          ORDER BY ESTADO ASC;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "df_estado_id = df_estado.withColumn(\"ID_ESTADO\", monotonically_increasing_id() +1) \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "CREATE TABLE DW.DIM_ESTADO(\n",
    "ID_ESTADO NUMERIC(2) NOT NULL,\n",
    "ESTADO VARCHAR(2) NULL,\n",
    "PRIMARY KEY (ID_ESTADO));\n",
    "\n",
    "'''\n",
    "\n",
    "display(df_estado_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3283ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "carga_dados(df_estado_id, \"DW.DIM_ESTADO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_municipio = spark.sql(\"\"\" SELECT DISTINCT MUNICIPIO FROM ANP_NEW\n",
    "                          ORDER BY MUNICIPIO ASC;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "df_municipio_id = df_municipio.withColumn(\"ID_MUNICIPIO\", monotonically_increasing_id() +1) \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "CREATE TABLE DW.DIM_MUNICIPIO(\n",
    "ID_MUNICIPIO NUMERIC(3) NOT NULL,\n",
    "ESTADO VARCHAR(30) NULL,\n",
    "PRIMARY KEY (ID_MUNICIPIO));\n",
    "\n",
    "'''\n",
    "\n",
    "display(df_municipio_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "carga_dados(df_municipio_id, \"DW.DIM_MUNICIPIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d1d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_revenda = spark.sql(\"\"\" SELECT DISTINCT REVENDA, CNPJ \n",
    "                           FROM ANP_NEW\n",
    "                           ORDER BY CNPJ ASC;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "df_revenda_id = df_revenda.withColumn(\"ID_REVENDA\", monotonically_increasing_id() +1) \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "CREATE TABLE DW.DIM_REVENDA(\n",
    "ID_REVENDA NUMERIC(5) NOT NULL,\n",
    "REVENDA VARCHAR(110) NULL,\n",
    "CNPJ VARCHAR(30) NULL,\n",
    "PRIMARY KEY (ID_REVENDA));\n",
    "\n",
    "'''\n",
    "\n",
    "display(df_revenda_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "carga_dados(df_revenda_id, \"DW.DIM_REVENDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c67fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_produto = spark.sql(\"\"\" SELECT DISTINCT PRODUTO\n",
    "                           FROM ANP_NEW\n",
    "                           WHERE PRODUTO IS NOT NULL\n",
    "                           ORDER BY PRODUTO ASC;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "df_produto_id = df_produto.withColumn(\"ID_PRODUTO\", monotonically_increasing_id() +1) \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "CREATE TABLE DW.DIM_PRODUTO(\n",
    "ID_PRODUTO NUMERIC(2) NOT NULL,\n",
    "PRODUTO VARCHAR(20) NULL,\n",
    "PRIMARY KEY (ID_PRODUTO));\n",
    "\n",
    "'''\n",
    "\n",
    "display(df_produto_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "carga_dados(df_produto_id, \"DW.DIM_PRODUTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebaa8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_bandeira = spark.sql(\"\"\" SELECT DISTINCT BANDEIRA\n",
    "                           FROM ANP_NEW\n",
    "                           WHERE BANDEIRA IS NOT NULL\n",
    "                           ORDER BY BANDEIRA ASC;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "df_bandeira_id = df_bandeira.withColumn(\"ID_BANDEIRA\", monotonically_increasing_id() +1) \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "CREATE TABLE DW.DIM_BANDEIRA(\n",
    "ID_BANDEIRA NUMERIC(2) NOT NULL,\n",
    "BANDEIRA VARCHAR(30) NULL,\n",
    "PRIMARY KEY (ID_BANDEIRA));\n",
    "\n",
    "'''\n",
    "\n",
    "display(df_bandeira_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc150002",
   "metadata": {},
   "outputs": [],
   "source": [
    "carga_dados(df_bandeira_id, \"DW.DIM_BANDEIRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_unid_med= spark.sql(\"\"\" SELECT DISTINCT UNID_MED\n",
    "                           FROM ANP_NEW\n",
    "                           WHERE UNID_MED IS NOT NULL\n",
    "                           ORDER BY UNID_MED ASC;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "df_unid_med_id = df_unid_med.withColumn(\"ID_UNID_MED\", monotonically_increasing_id() +1) \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "CREATE TABLE DW.DIM_UNID_MED(\n",
    "ID_UNID_MED NUMERIC(2) NOT NULL,\n",
    "UNID_MED VARCHAR(30) NULL,\n",
    "PRIMARY KEY (ID_UNID_MED));\n",
    "\n",
    "'''\n",
    "\n",
    "display(df_unid_med_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd820ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "carga_dados(df_unid_med_id, \"DW.DIM_UNID_MED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86710f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_endereco= spark.sql(\"\"\" SELECT DISTINCT NOME_RUA,\n",
    "                                           NUM_RUA,\n",
    "                                           COMPL_RUA,\n",
    "                                           BAIRRO,\n",
    "                                           CEP\n",
    "                                           FROM ANP_NEW\n",
    "                                           ORDER BY CEP ASC;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "df_endereco_id = df_endereco.withColumn(\"ID_ENDERECO\", monotonically_increasing_id() +1) \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "CREATE TABLE DW.DIM_ENDERECO(\n",
    "ID_ENDERECO NUMERIC(6) NOT NULL,\n",
    "NOME_RUA VARCHAR(70) NULL,\n",
    "NUM_RUA VARCHAR(12) NULL,\n",
    "COMPL_RUA VARCHAR(128) NULL, \n",
    "BAIRRO VARCHAR(48) NULL,\n",
    "CEP VARCHAR(10) NULL,\n",
    "PRIMARY KEY (ID_ENDERECO));\n",
    " \n",
    "'''\n",
    "\n",
    "display(df_endereco_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b15051",
   "metadata": {},
   "outputs": [],
   "source": [
    "carga_dados(df_endereco_id, \"DW.DIM_ENDERECO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4952ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABELA FATO - STAR SCHEMA\n",
    "\n",
    "df_dm_endereco = df_endereco_id.createOrReplaceTempView(\"DIM_ENDERECO\")\n",
    "df_dm_revenda = df_revenda_id.createOrReplaceTempView(\"DIM_REVENDA\")\n",
    "df_dm_estado = df_estado_id.createOrReplaceTempView(\"DIM_ESTADO\")\n",
    "df_dm_municipio = df_municipio_id.createOrReplaceTempView(\"DIM_MUNICIPIO\")\n",
    "df_dm_regiao = df_regiao_id.createOrReplaceTempView(\"DIM_REGIAO\")\n",
    "df_dm_produto = df_produto_id.createOrReplaceTempView(\"DIM_PRODUTO\")\n",
    "df_dm_bandeira = df_bandeira_id.createOrReplaceTempView(\"DIM_BANDEIRA\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "FUNÇÃO PARA CRIAR VIEWS DAS DIMENSÕES NO DATABRICKS - SIMULANDO UM AMBIENTE REAL - PEGANDO DO SQL SERVER\n",
    "\n",
    "def leituraDB(query):\n",
    "    df_leitura = spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\",\"jdbc:sqlserver://projeto-eng-dados-sqlserver.database.windows.net\")\\\n",
    "    .option(\"port\",\"1433\")\\\n",
    "    .option(\"user\", \"gibran\")\\\n",
    "    .option(\"password\",\"Blackggs18#3\")\\\n",
    "    .option(\"database\",\"projeto-eng-dados-databaseql\")\\\n",
    "    .option(\"query\", query)\\\n",
    "    .load()\n",
    "\n",
    "return df_leitura   \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "CRIANDO AS VIEWS COM A FUNÇÃO leituraDB() - PEGANDO A TABELA DO SQL SERVER\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "df_dm_endereco = leituraDB(\"SELECT * FROM DW.DIM_ENDERECO\")\n",
    "\n",
    "df_dm_endereco.createOrReplaceTempView(\"DIM_ENDERECO\")\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função manual \n",
    "\n",
    "df_med = spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\",\"jdbc:sqlserver://projeto-eng-dados-sqlserver.database.windows.net\")\\\n",
    "    .option(\"port\",\"1433\")\\\n",
    "    .option(\"user\", \"gibran\")\\\n",
    "    .option(\"password\",\"Blackggs18#3\")\\\n",
    "    .option(\"database\",\"projeto-eng-dados-databaseql\")\\\n",
    "    .option(\"query\", \"SELECT * FROM DW.DIM_UNID_MED\")\\\n",
    "    .load()\n",
    "\n",
    "\n",
    "\n",
    "df_dm_unidade_med = df_med.createOrReplaceTempView(\"DIM_UNIDADE_MED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiramente faz se uma filtragem para não trazer dados nulos \n",
    "\n",
    "df_fato_filtro = spark.sql(\"\"\" SELECT * FROM ANP_NEW \n",
    "                               WHERE PRODUTO IS NOT NULL\n",
    "                               AND REVENDA IS NOT NULL\n",
    "                               AND DT_COLETA IS NOT NULL\n",
    "                               AND VL_VENDA IS NOT NULL\n",
    "                               AND VL_COMPRA IS NOT NULL;\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "display(df_fato_filtro)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebda5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fato_filtro.createOrReplaceTempView(\"FATO_FILTRO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0bbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela Fato vindo de FATO_FILTRO\n",
    "\n",
    "df_tab_fato = spark.sql(\"\"\" SELECT A.DT_COLETA,\n",
    "                            A.VL_VENDA,\n",
    "                            A.VL_COMPRA,\n",
    "                            B.ID_REVENDA,\n",
    "                            C.ID_PRODUTO,\n",
    "                            D.ID_ESTADO,\n",
    "                            E.ID_MUNICIPIO,\n",
    "                            F.ID_REGIAO,\n",
    "                            G.ID_UNID_MED,\n",
    "                            H.ID_BANDEIRA,\n",
    "                            I.ID_ENDERECO\n",
    "                        FROM FATO_FILTRO AS A\n",
    "                        LEFT JOIN DIM_REVENDA AS B\n",
    "                        ON A.REVENDA = B.REVENDA\n",
    "\n",
    "                        LEFT JOIN DIM_PRODUTO AS C\n",
    "                        ON A.PRODUTO = C.PRODUTO\n",
    "\n",
    "                        LEFT JOIN DIM_ESTADO AS D\n",
    "                        ON A.ESTADO = D.ESTADO \n",
    "\n",
    "                        LEFT JOIN DIM_MUNICIPIO AS E\n",
    "                        ON A.MUNICIPIO = E.MUNICIPIO\n",
    "\n",
    "                        LEFT JOIN DIM_REGIAO AS F\n",
    "                        ON A.REGIAO = F.REGIAO\n",
    "\n",
    "                        LEFT JOIN DIM_UNIDADE_MED AS G\n",
    "                        ON A.UNID_MED = G.UNID_MED\n",
    "\n",
    "                        LEFT JOIN DIM_BANDEIRA AS H\n",
    "                        ON A.BANDEIRA = H.BANDEIRA \n",
    "\n",
    "                        LEFT JOIN DIM_ENDERECO AS I\n",
    "                        ON A.CEP = I.CEP;\n",
    "\"\"\")\n",
    "\n",
    "display(df_tab_fato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d070a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAVANDO A TABELA FATO NO DATA LAKE\n",
    "\n",
    "df_tab_fato.coalesce(1)\\\n",
    "    .write\\\n",
    "    .format('parquet')\\\n",
    "    .mode('overwrite')\\\n",
    "    .save('/mnt/projetodados/refined/fato', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c921d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mover_arquivo(extensao, origem, destino, novo_nome):\n",
    "    files = dbutils.fs.ls(origem)\n",
    "    for i in range(0, len(files)):\n",
    "        file = files[i].name\n",
    "\n",
    "        if extensao in file:\n",
    "            dbutils.fs.cp(files[i].path, destino + novo_nome + extensao)\n",
    "            print('Origem é: ' + file + ' ' + 'Destino é: ' + destino + novo_nome + extensao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd24759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mover_arquivo('.parquet', '/mnt/projetodados/refined/fato/', '/mnt/projetodados/refined/fato_anp/', 'tb_fato_anp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c0ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "/*\n",
    "CREATE TABLE DW.FATO_ANP(\n",
    "DT_COLETA DATE NOT NULL,\n",
    "VL_COMPRA NUMERIC(4,3) NOT NULL,\n",
    "VL_VENDA NUMERIC(4,3) NOT NULL,\n",
    "ID_REVENDA NUMERIC(5) NOT NULL,\n",
    "ID_PRODUTO NUMERIC(5) NOT NULL,\n",
    "ID_ESTADO NUMERIC(5) NOT NULL,\n",
    "ID_MUNICIPIO NUMERIC(5) NOT NULL,\n",
    "ID_REGIAO NUMERIC(5) NOT NULL,\n",
    "ID_UNID_MED NUMERIC(5) NOT NULL,\n",
    "ID_BANDEIRA NUMERIC(5) NOT NULL,\n",
    "ID_ENDERECO NUMERIC(5)NOT NULL);\n",
    "\n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
